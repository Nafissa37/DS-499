---
title: "Pittsburgh Trees"
author: "Nafissa Bia"
date: "May 6, 2025"
output:
    prettydoc::html_pretty:
    theme: hpstr
    toc: yes
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r eval = FALSE}
install.packages(prettydoc)
```

```{r load-libraries}
library(tidyverse)
library(caret)
library(readr)
library(ranger)
library(vip)
library(rsample)
```

```{r load-data}
trees <- read_csv("data/trees.csv", na=c("", "N/A"))
```

## Introduction ##
This paper analyzes data on trees from Pittsburgh, Pennsylvania. The dataset was obtained from [Data.gov](https://catalog.data.gov/dataset/city-of-pittsburgh-trees) and contains detailed information about trees maintained by The City of Pittsburgh Department of Public Works Forestry Division. The dataset consists of `r nrow(trees)` rows and `r ncol(trees)` columns, with each row representing a single tree located within the city’s boundaries. The data was collected through physical inspections and monitoring efforts and was last updated in 2020.


```{r echo = FALSE, out.height="40%", out.width = "60%", fig.align = "center"}
knitr::include_graphics("img/pittsburgh_trees.jpg")
```

Each tree is identified by location-specific details, such as `address_number` and `street_name`, and geographic coordinates, such as`longitude` and `latitude`. The dataset includes categorical variables, like `street`, `common_name`, `scientific_name`, and `growth_space_type` ('Well or Pit', 'Open or Unrestricted', etc.) that describe the tree’s location, identity, and growing conditions. Other categorical variables, such as `land_use` ('Residential', 'Vacant', etc.), indicate the type of property on which the tree is situated.

There are also several continuous variables that characterize the physical dimensions and environmental benefits provided by each tree. These include measurements like `height`, `width`, `growth_space_length`, `growth_space_width`, and `diameter_base_height`, which give information on the tree’s size and growth space. Environmental benefits, such as stormwater management and air quality improvement, are quantified through variables like `co2_benefits_maint_lbs`, `air_quality_benfits_so2dep_lbs`, and `stormwater_benefits_runoff_elim.` Additionally, variables like `energy_benefits_electricity_dollar_value` and `energy_benefits_gas_dollar_value` estimate energy savings provided by each tree in terms of reducing electricity and gas usage. 

All the tree benefits from this dataset were calculated using the [National Tree Benefit Calculator Web Service](http://www.treebenefits.com/calculator/webservicedescription.cfm), which estimates the environmental benefits of individual trees based on their species, size (e.g diameter at breast height), and location/regional climate. It uses scientific models developed by the U.S. Forest Service and other research institutions to quantify benefits like carbon dioxide reduction, stormwater management, or air quality improvement in units such as pounds, gallons, or dollars (to represent the financial value of the ecosystem services provided by these trees).

Overall, this dataset gives a comprehensive view of the value and condition of Pittsburgh's urban forest.


## Research Questions ##

After an initial study of the variables contained in this dataset, here are a few questions of interest:

- Which factors can help predict the amount of stormwater runoff absorbed by trees?
- Can tree dimensions predict their impact on air quality?
- Can we classify trees as having overhead utilities or not based on their dimensions or the benefits they provide?
- Can the CO$_2$ benefits provided by a tree help classify the tree as being located in a residential area versus an industrial area?


```{r data-filtering}
trees_clean <- trees %>%
  rename(stormwater_benefits = stormwater_benefits_dollar_value, 
         stormwater_elimination = stormwater_benefits_runoff_elim,
         electricity_benefits = energy_benefits_electricity_dollar_value,
         gas_benefits = energy_benefits_gas_dollar_value,
         total_air_benefits = air_quality_benfits_total_dollar_value,
         sequestred_CO2 = co2_benefits_sequestered_lbs,
         total_benefits = overall_benefits_dollar_value) %>%
  select(-`_id`) %>%
  filter(stormwater_elimination < 10000 & height < 125) %>%
  mutate(overhead_numeric = if_else(overhead_utilities == "Yes", 1, 0),
    overhead_numeric = as.factor(overhead_numeric),
    growth_space_type = case_when(
      growth_space_type == "Well or Pit" ~ "Well/Pit",
      growth_space_type == "Open or Unrestricted" ~ "Open",
      growth_space_type == "Open or Restricted" ~ "Restricted",
      growth_space_type == "Tree Lawn or Parkway" ~ "Tree Lawn",
      TRUE ~ growth_space_type),
    overhead_utilities = case_when(
      overhead_utilities == "Conflicting" ~ "Yes",
      TRUE ~ overhead_utilities),
    growth_space_area = growth_space_length * growth_space_width,
    condition = fct_relevel(condition, "Excellent", "Very Good", "Good", 
                                 "Fair", "Poor", "Critical", "Dead")) 
```

```{r}
trees_clean <- na.omit(trees_clean)
```


```{r data-splitting}
set.seed(1234)

# 80/20 split 
trees_split <- initial_split(trees_clean, prop = 0.80)
train_data <- training(trees_split)
test_data <- testing(trees_split)
```


## Exploratory Data Analysis ##

**Note that observations with missing values have been omitted, and the data has been split into training data and testing data using an 80/20 split.**
 

```{r eval = FALSE}
train_data %>%
  count(neighborhood) %>%
  arrange(desc(n))
```

- **Exploring Quantitative Variables**: For some numeric variables like `height`, `diameter_base_height`, and `overall_benefits_dollar_value`, plotting histograms and boxplots showed unimodal right-skewed distributions with many high outliers and relatively small IQR values. Some of these graphs are shown below. 
  
```{r}
# Height
train_data %>%
  ggplot(aes(x = height)) +
  geom_boxplot(color = "#7cc560") +
  labs(title = "Distribution of Height", x = "Height in feet") +
theme(axis.ticks.y = element_blank(),
      axis.text.y = element_blank())
train_data %>%
  ggplot(aes(x = height)) +
  geom_histogram(binwidth = 10, fill = "#7cc560", color = "black") +
  labs(title = "Distribution of Height", x = "Height in feet", y = "Count") 
```

```{r}
summary(train_data$height)
```
```{r eval = FALSE}
# Width
train_data %>%
  ggplot(aes(x = width)) +
  geom_boxplot(color = "#7cc560") +
  labs(title = "Distribution of Width", x = "Width in feet") +
theme(axis.ticks.y = element_blank(),
      axis.text.y = element_blank())
```

```{r eval = FALSE}
summary(train_data$width)
```


```{r eval = FALSE}
# Diameter Base Height
train_data %>%
  ggplot(aes(x = diameter_base_height)) +
  geom_boxplot(color = "#7cc560") +
  labs(title = "Distribution of Diameter Base Height", x = "Diameter Base Height in feet") +
theme(axis.ticks.y = element_blank(),
      axis.text.y = element_blank())
```

```{r eval =  FALSE}
summary(train_data$diameter_base_height)
```


```{r eval = FALSE}
# Total CO2 Benefits
train_data %>%
  ggplot(aes(x = co2_benefits_totalco2_lbs)) +
  geom_boxplot(color = "#7cc560") +
  labs(title = "Distribution of Total CO2 Benefits", x = "Total CO2 Benefits in lbs") +
theme(axis.ticks.y = element_blank(),
      axis.text.y = element_blank())
```

```{r eval = FALSE}
summary(train_data$co2_benefits_totalco2_lbs)
```


``` {r eval=FALSE}
# Total Benefits
train_data %>%
  ggplot(aes(x = total_benefits)) +
  geom_boxplot(color = "#7cc560") +
  labs(title = "Distribution of Total Benefits", y = "Total Benefits") +
  scale_x_continuous(labels = scales::label_dollar()) 
train_data %>%
  ggplot(aes(x = total_benefits)) +
  geom_histogram(binwidth = 25, fill = "#7cc560", color = "black") +
  labs(title = "Distribution of Total Benefits", x = "Total Benefits", y = "Count") +
  scale_x_continuous(labels = scales::label_dollar())
```
```{r eval=FALSE}
summary(train_data$total_benefits)
```

```{r eval =  FALSE}
train_data %>%
  filter(total_benefits < 0) %>%
  select(common_name, scientific_name, total_benefits)
```

 - **Variable and value names**
    - Some of the variable names are slightly lengthy, especially the variables about environmental benefits. Thus these variables have been recoded to have shorter names.

    - For some of the categorical variables, there are inconsistencies in the names of the values. For instance, for the variable `growth_space_type`, there were values such as ‘Well or Pit’ and ‘Well/Pit’ or ‘Tree Lawn’ and ‘Tree Law or Parkway’, which are recoded to be in the same categories. Additionally, the variable `overhead_utilities` had the options 'Yes', 'No', and 'Conflicting'. These values have been recoded so that 'Yes' and 'Conflicting' are 'Yes' and 'No' is 'No'.

```{r eval = FALSE}
train_data %>%
  distinct(land_use)
```

```{r eval = FALSE} 
train_data %>%
  distinct(condition)
```

  - **Exploring categorical variables** 

Here are the distributions of some categorical variables in this dataset:

```{r eval = FALSE}
train_data %>%
  count(common_name) %>%
  arrange(desc(common_name))
```


```{r eval=FALSE}
# Growth Space Type
train_data %>%
  ggplot(aes(x = fct_infreq(growth_space_type))) +
  geom_bar(fill = "#7cc560") +
  labs(title = "Distribution of Growth Space Type", 
       x = "", y = "Count")
```


```{r}
# Presence of Overhead Utilities
train_data %>%
  ggplot(aes(x = overhead_utilities)) +
  geom_bar(fill = "#7cc560") +
  labs(title = "Distribution of Overhead Utilities Presence", 
       x = "", y = "Count")
```

*There are many trees both with and without overhead utilities, but most trees do not have overhead utilities.*

```{r}
# Land Use
train_data %>%
  ggplot(aes(y = fct_infreq(land_use))) +
  geom_bar(fill = "#7cc560") +
  labs(title = "Distribution of Land Use", 
       x = "Count", y = "")
```

*The most common land use category for trees in this dataset is 'Residential', followed by 'Commercial/Industrial' and 'Vacant'. There are relatively very small numbers of trees in other types of land use, such as 'Water/Wetland', 'Golf Course', and 'Agriculture'.*

```{r eval=FALSE}
train_data %>%
  ggplot(aes(x = fct_infreq(condition))) +
  geom_bar(fill = "#7cc560") +
  labs(title = "Distribution of Tree Condition", 
       x = "", y = "Count")
```


## Exploring Research Questions ## 

We are now going to explore our questions of interest using random forest models, for the following reasons:

- Compared to linear or logistic regression models, they can can handle more complex patterns in the data, including non-linear relationships and interactions between variables.
- They are much less sensitive to outliers, which is especially helpful here given the large number of extreme values in the dataset.

- They provide built-in variable importance scores, which make it easier to answer the research questions about the most impactful features on tree properties.

### 1. Which factors can help predict the amount of stormwater runoff absorbed by trees? ###

#### &#9733; Random Forest Model ####

```{r eval=FALSE}
#Number of features
n_features <- length(setdiff(names(train_data), c("id", "address_number", "street", "longitude", "latitude", "stormwater_elimination", "stormwater_benefits")))


rf_stormwater <- ranger(
  formula = stormwater_elimination ~ . -id -address_number -street -longitude -latitude -stormwater_benefits,
  data = train_data,
  mtry = floor(n_features / 3), #number of features per split
  respect.unordered.factors = "order",  
  seed = 123,
  importance = "impurity"
)
rf_stormwater

#Save random forest model
write_rds(rf_stormwater, "rf_stormwater.rds")
```

```{r}
#Load saved model
rf_stormwater <- read_rds("rf_stormwater.rds")

rf_stormwater
```


#### &#9733; Model Performance Evaluation ####

- Variable Importance Plot

```{r}
#Variable importance plot
p <- vip::vip(rf_stormwater, num_features = 10)

# Modify the fill color using ggplot2 
p + 
  geom_col(fill = "#7cc560") +
  theme_minimal()
```

*Many variables related to air quality benefits have high importance for predicting the amount of stormwater eliminated by a tree. Some of these variables include the ones related to the amount of PM10 or CO~2~ eliminated by a tree, the amount of Ozone or Nitrogen Dioxide deposited by a tree, and the total air benefits provided by a tree.* 

- RMSE

```{r}
# get OOB RMSE
(default_rmse <- sqrt(rf_stormwater$prediction.error))
```
*The RMSE is approximately 31.05 gallons, which seems pretty good since the average stormwater elimination of a tree is 1115 gallons, so the model's predictions are relatively close to the actual values.*


### 2. Can tree dimensions help predict their impact on air quality? ### 

#### &#9733; Random Forest Model ####

```{r eval=FALSE}
# Find all columns that start with "air"
air_benefits_columns <- grep("^air", names(train_data), value = TRUE)
CO2_benefits_columns <- grep("^co2", names(train_data), value = TRUE)

#Number of features
n_features <- length(setdiff(names(train_data), c("id", "address_number", "street", "longitude", "latitude", "total_air_benefits", air_benefits_columns, "sequestred_CO2", CO2_benefits_columns)))


#Remove appropriate columns from data set
#("-air_benefits_columns" in 'formula' doesn't work)
#similar issue with 'select'
#Also remove columns related to CO2 benefits

train_data_air <- train_data[, !(names(train_data) %in% c("id", "address_number", "street", "longitude", "latitude", "sequestred_CO2", air_benefits_columns, CO2_benefits_columns))]
  
rf_air_benefits <- ranger(
  formula = total_air_benefits ~ .,
  data = train_data_air,
  mtry = floor(n_features / 3), #number of features per split for regression
  respect.unordered.factors = "order",  
  seed = 123,
  importance = "impurity"
)
rf_air_benefits

#Save random forest model
write_rds(rf_air_benefits, "rf_air_benefits.rds")
```

```{r}

#Load saved model
rf_air_benefits <- read_rds("rf_air_benefits.rds")

rf_air_benefits
```


#### &#9733; Model Performance Evaluation ####

- Variable Importance Plot

```{r}
#Variable importance plot
p <- vip::vip(rf_air_benefits, num_features = 10)

# Modify the fill color using ggplot2 
p + 
  geom_col(fill = "#7cc560") +
  theme_minimal()
```

*Some variables related to tree dimensions, such as `diameter_base_height`, `height`, or `property_value_benefits_leaf_surface_area` (leaf surface area in square feet contributing to property value benefits), have relatively high importance in predicting tree impact on air quality. Other variables with high importance include `stormwater_benefits`, `tree condition`, and `scientific name`.*


```{r}
# get OOB RMSE
(default_rmse <- sqrt(rf_air_benefits$prediction.error))
```
*The RMSE is approximately $0.16, which seems pretty good since the average monetary value of the total air benefits provided by a tree is approximately $7, so the model's predictions are relatively close to the actual values.*

```{r eval=FALSE}
mean(train_data$total_air_benefits)
```


### 3. Can we classify trees as having overhead utilities or not based on their dimensions or the benefits they provide? ### 

#### &#9733; Random Forest Model ####

```{r eval=FALSE}
#Number of features
n_features <- length(setdiff(names(train_data), c("id", "address_number", "street", "longitude", "latitude", "overhead_utilities", "overhead_numeric")))

rf_overhead1 <- ranger(
  formula = overhead_numeric ~ . -id -address_number -street -longitude -latitude -overhead_utilities,
  data = train_data,
  # mtry = floor(n_features / 3),  #We get the default for classification (sqrt(n_features))
  respect.unordered.factors = "order",  
  seed = 123,
  importance = "impurity",
)

rf_overhead1

#Save random forest model
write_rds(rf_overhead1, "rf_overhead1.rds")
```

```{r}
#Load saved model
rf_overhead1 <- read_rds("rf_overhead1.rds")

rf_overhead1
```


#### &#9733; Model Performance Evaluation ####

- Variable Importance Plot

```{r}
#Variable importance plot
p <- vip::vip(rf_overhead1, num_features = 10)

# Modify the fill color using ggplot2 
p + 
  geom_col(fill = "#7cc560") +
  theme_minimal()
```

*The most important variables for classifying a tree as having overhead utilities include `fire_zone` (the fire zone where the tree is located), `neighborhood`, `scientific_name`, and `tract` (census tract where the tree is located). Some variables related to tree dimensions, like `height`, `width`, and `growth_space_width`, also show up as important. However, most benefit-related variables don’t seem to contribute much to the prediction, although a few related to property value benefits do show up among the top 20 features.*

- Confusion Matrix

```{r}
pred_test <- predict(rf_overhead1, test_data, type="response")
confusionMatrix(pred_test$predictions, test_data$overhead_numeric, positive = "1")
```
*The accuracy rate for this model is 78.37% (compared to 62.44% with logistic regression), so the model got 78.37% of the predictions overall right, which is not bad. The sensitivity is 59.76% (59.76% of the trees that do have overhead utilities were correctly identified) while the specificity is 88.03% (88.03% of the trees that do not have overhead utilities were correctly identified), so the model is much better at recognizing trees without overhead utilities (less false positives and more false negatives).*


### 4. Can the CO$_2$ benefits provided by a tree help classify the tree as being located in a residential area versus an industrial area? ### 

#### &#9733; Random Forest Model ####

```{r}
#Focusing only on 2 categories for land_use (Residential and Industrial)
train_data_land_use <- train_data %>%
  filter(land_use == "Residential" | land_use == "Commercial/Industrial") %>%
  mutate(land_use = ifelse(land_use == "Commercial/Industrial", "Industrial", land_use))

test_data_land_use <- test_data %>%
  filter(land_use == "Residential" | land_use == "Commercial/Industrial") %>%
  mutate(land_use = ifelse(land_use == "Commercial/Industrial", "Industrial", land_use))
```


```{r eval=FALSE}
train_data_land_use$land_use <- as.factor(train_data_land_use$land_use)
test_data_land_use$land_use <- as.factor(test_data_land_use$land_use)

#Number of features
n_features <- length(setdiff(names(train_data), c("id", "address_number", "street", "longitude", "latitude", "land_use")))

#Remove neighborhood as well?
rf_land_use1 <- ranger(
  formula = land_use ~ . -id -address_number -street -longitude -latitude,
  data = train_data_land_use,
  # mtry = floor(n_features / 3),  #We get the default for classification (sqrt(n_features))
  respect.unordered.factors = "order",  
  seed = 123,
  importance = "impurity",
)

rf_land_use1

#Save random forest model
write_rds(rf_land_use1, "rf_land_use1.rds")
```

```{r}
#Load saved model
rf_land_use1 <- read_rds("rf_land_use1.rds")

rf_land_use1
```

#### &#9733; Model Performance Evaluation ####


- Variable Importance Plot


```{r}
#Variable importance plot
p <- vip::vip(rf_land_use1, num_features = 10)

# Modify the fill color using ggplot2 
p + 
  geom_col(fill = "#7cc560") +
  theme_minimal()
```

*The most important variables for classifying a tree by location type include `fire_zone`, `neighborhood`, `scientific_name`, `tract`, and many variables related to tree's growth space. The CO~2~ benefits variables don't seem to be very important in predicting its location type (residential vs industrial), but they do show up among the top 20 features.*

- Confusion Matrix

```{r}
# Get predictions
pred_test <- predict(rf_land_use1, test_data_land_use, type = "response")

# Get the actual and predicted classes
actuals <- test_data_land_use$land_use
preds <- factor(pred_test$predictions, levels = levels(actuals))

# Drop unused levels to avoid mismatch
actuals <- factor(actuals)
preds <- factor(preds, levels = levels(actuals))

# Compute confusion matrix
confusionMatrix(preds, actuals)
```

*The accuracy rate for this model is 94.97% (compared to 83.3% with logistic regression), so the model got 94.97% of the predictions overall right, which is pretty good. The sensitivity is 78.72% (78.72% of the trees that are located in an industrial area were correctly identified) while the specificity is 98.45% (98.45% of the trees that are located in a residential area were correctly identified), so the model is better at recognizing trees located in a residential area.*


## Conclusion ##

This paper explored Pittsburgh’s urban forest by applying random forest models to identify which tree features best predict environmental benefits and classification outcomes. 

Models predicting the amount of stormwater elimination and air quality benefits performed well, with low RMSE values relative to average benefit levels. Tree dimensions, such as height, diameter, and growth space, and air quality variables were among the top predictors. 

For classification questions, like determining the presence of overhead utilities or distinguishing between residential and industrial land use, spatial variables (e.g. neighborhood, fire zone, census tract) were more important. Environmental benefit variables played a smaller role in these models, though some, like property value benefits, still appeared as moderately important features.

Overall, structural and spatial features were more useful for classification, while benefit-related variables were more informative in regression. 

Beyond model performance, the research questions can help inform practical decisions in city planning. For example, identifying which trees offer the most environmental benefits can help prioritize planting in high-need areas. Classifying trees with overhead utilities could improve safety and maintenance planning, and patterns across land use types may point out areas lacking in tree-related benefits. Altogether, these findings can support more data-informed decisions about where and how to manage trees across different neighborhoods in Pittsburgh.
